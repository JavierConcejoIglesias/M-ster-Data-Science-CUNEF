{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b07be3",
   "metadata": {},
   "source": [
    "# Francisco Javier Concejo Iglesias\n",
    "# Análisis de información no estructurada\n",
    "# Actividad 2 Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d782628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ceea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import spacy\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp= spacy.load('en_core_web_lg')\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d62a3b",
   "metadata": {},
   "source": [
    "## 1. Lea el contenido del fichero csv en un DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f8cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('C:/Users/javoc/OneDrive/Escritorio/MASTER/AnalisisDeInformacionNoEstructurada/Practica2/spam.csv')\n",
    "print(data.shape)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3931469",
   "metadata": {},
   "source": [
    "Nuestros datos están compuestos por 5572 filas y 2 columnas. Las filas representan los comentarios tanto de spam como de no spam. La primera columna es la categoría de el comentario y la segunda el propio comentario. La categoría puede tomar dos valores: spam si es un mensaje de spam y ham si es un mensaje de un usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109fbc2",
   "metadata": {},
   "source": [
    "## 2. Realice el pre-procesamiento que considere necesario. Puede utilizar funciones de la librería NLTK o spaCy, a su voluntad. Recomendamos una escritura modular del código, para poder hacer pruebas posteriormente, y contestar a las preguntas del punto 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c7d28",
   "metadata": {},
   "source": [
    "#### Eliminación de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16456565",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "datos_stopwords=list(data[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e9024",
   "metadata": {},
   "source": [
    "#### Extracción de las formas canónicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2640474",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=list(data['text'])\n",
    "formas_canonicas=[]\n",
    "for x in lista:\n",
    "    element=nlp(x)\n",
    "    x=''\n",
    "    for z in element:\n",
    "        x+=' '+z.lemma_.lower()\n",
    "    formas_canonicas.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f8391",
   "metadata": {},
   "source": [
    "El pre-procesamiento del texto para esta práctica va a consistir en dos partes. En la primera de ellas eliminamos las palabras vacías (stopwords) que carecen de significado. La segunda va a consistir en la extracción de la forma canónica de cada una de las palabras de nuestro corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd8826",
   "metadata": {},
   "source": [
    "## 3. Divida el conjunto de documentos en un subconjunto de entrenamiento y otro de evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefa0efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4457, 2)\n",
      "Test: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "train, test = temporal_train_test_split(y = data, train_size=0.8)\n",
    "print('Train:', train.shape)\n",
    "print('Test:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f58ef",
   "metadata": {},
   "source": [
    "## 4. Convierta el corpus de documentos en una matriz TF-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8d4f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "      <th>žö</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "\n",
       "   0125698789   02  ...   û_  û_thanks  ûªm  ûªt  ûªve   ûï  ûïharry   ûò  \\\n",
       "0         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "1         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "2         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "3         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "4         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "\n",
       "   ûówell   žö  \n",
       "0     0.0  0.0  \n",
       "1     0.0  0.0  \n",
       "2     0.0  0.0  \n",
       "3     0.0  0.0  \n",
       "4     0.0  0.0  \n",
       "\n",
       "[5 rows x 8673 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "def tfidf(datos):\n",
    "    TFIDF_vectorizer = TfidfVectorizer()\n",
    "    TFIDF_array=TFIDF_vectorizer.fit_transform(datos).toarray()\n",
    "    vocab_TFIDF = TFIDF_vectorizer.get_feature_names()\n",
    "    TFIDF=pd.DataFrame(TFIDF_array, columns=vocab_TFIDF)\n",
    "    return(TFIDF).head(5)\n",
    "\n",
    "tfidf(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bc8de",
   "metadata": {},
   "source": [
    "La matriz tfidf resuelve el principal problema que tiene la BOW de quitar importancia a aquellos términos que son menos frecuentes pero que pueden ser relevantes. Para resolver dicho problema, realiza una normalización.\n",
    "\n",
    "Realizamos la matriz tfidf y observamos que hay muchos términos (columnas) los cuales, a priori, carecen de significado o contienen caracteres extraños. Sin embargo, no he decidido aplicar el parámetro \"max_features\" ya que este lo aplicaremos posteriormente en el ejercicio 6. Por lo tanto, podemos concluir que es necesario realizar un pre-procesamiento ya que no podemos obtener información de esta matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8431d",
   "metadata": {},
   "source": [
    "## 5. Llegados a este punto, realice modelos de entrenamiento al menos con algoritmos de clasificador bayesiano ingenuo, máquinas SVM y un modelo basado en árbol de decisión. Obtenga resultados de accuracy de la clasificación, así como las matrices de confusión para los tres modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6bf79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = data.iloc[:,1]\n",
    "label= data.iloc[:,0]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "texto_tfidf = vectorizer.fit_transform(texto)\n",
    "texto_tfidf_dense = texto_tfidf.todense()\n",
    "\n",
    "texto_train, texto_test, label_train, label_test = temporal_train_test_split(texto_tfidf, label, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33307079",
   "metadata": {},
   "source": [
    "#### Clasificador bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584d0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesiano = MultinomialNB()\n",
    "bayesiano.fit(texto_train,label_train)\n",
    "label_pred_bayesiano=bayesiano.predict(texto_test)\n",
    "matriz_confusión_bayesiano = pd.DataFrame(confusion_matrix(label_test,label_pred_bayesiano))\n",
    "matriz_confusión_bayesiano.columns =['No spam', 'Spam']\n",
    "matriz_confusión_bayesiano.index=['Predicción no spam','Predicción spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b34f0d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04552232",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(texto_train, label_train)\n",
    "label_pred_svm = svm.predict(texto_test)\n",
    "matriz_confusión_svm = pd.DataFrame(confusion_matrix(label_test, label_pred_svm))\n",
    "matriz_confusión_svm.columns =['No spam', 'Spam']\n",
    "matriz_confusión_svm.index=['Predicción no spam','Predicción spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce641ba",
   "metadata": {},
   "source": [
    "#### Arbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fe1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol = tree.DecisionTreeClassifier()\n",
    "arbol.fit(texto_train,label_train)\n",
    "label_pred_arbol = arbol.predict(texto_test)\n",
    "matriz_confusión_arbol = pd.DataFrame(confusion_matrix(label_test, label_pred_arbol))\n",
    "matriz_confusión_arbol.columns =['No Spam', 'Spam']\n",
    "matriz_confusión_arbol.index=['Predicción no spam','Predicción spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd46e66",
   "metadata": {},
   "source": [
    "#### Matrices de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506d053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\">Bayesiano</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No spam</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicción no spam</th>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicción spam</th>\n",
       "      <td>41</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\">SVM</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No spam</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicción no spam</th>\n",
       "      <td>966</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicción spam</th>\n",
       "      <td>17</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2 style=\"text-align: center;\">Árbol de decisión</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Spam</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicción no spam</th>\n",
       "      <td>957</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicción spam</th>\n",
       "      <td>22</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(matriz_confusión_bayesiano,matriz_confusión_svm,matriz_confusión_arbol, titles=['Bayesiano','SVM','Árbol de decisión'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1ece2",
   "metadata": {},
   "source": [
    "He decidido realizar los tres modelos de clasificación supervisada para poder obtener una mayor comparación. Antes de analizar los resultados de las matrices de confusión, me gustaría realizar una breve explicación del código. En primer lugar, segmentamos nuestros datos en dos variables, una la cual contiene los textos en sí y otra que nos indica la categoría para cada mensaje. En segundo lugar, dividimos cada una de las variables en una muestra de entrenamiento sobre la cual entrenamos nuestro modelo y una muestra de validación sobre la cual lo validamos. Posteriormente, realizamos los tres modelos los cuales siguen una misma estuctura:\n",
    "- Definición del modelo\n",
    "- Entrenamiento del modelo\n",
    "- Predicciones del modelo\n",
    "- Creación de la matriz de confusión en base a estas predicciones y los valores reales\n",
    "\n",
    "Como se puede observar, el modelo de clasificación bayesiana es más preciso a la hora de predecir los mensajes de spam que de verdad son spam. Por otra parte, el modelo SVM es el mejor a la hora de predecir los mensajes que realmente no son spam. Es importante destacar que los falsos positivos de nuestras matrices de confusión van a tener un impacto más negativo que los falsos negativos. Esto se debe a que los falsos positivos se refieren a aquellos mensajes que nuestro modelo detecta como spam pero en realidad no lo son. En caso de que, por ejemplo, estemos analizando la satisfacción de los usuarios en base a los comentarios, estos se ignorarían ya que nuestro modelo estaría indicando que son spam y por lo tanto no se tendría en cuenta información de usuarios reales que podría ser relevante. Por lo tanto, en términos de falsos positivos el clasificador bayesiano sería el mejor ya que no tiene. Pese a que ninguno de los modelos tiene el mayor número de verdaderos positivos y el menor de verdaderos negativos, no se puede concluir a simple vista cual es el mejor. Es por eso por lo que calculamos el accuracy para los 3 modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aac908",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037a034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El accuracy del clasificador bayesiano es de 0.96\n",
      "El accuracy del SVM es de 0.98\n",
      "El accuracy del arbol de decisión es de 0.97\n"
     ]
    }
   ],
   "source": [
    "def accuracy(df):\n",
    "    TP=df.iloc[0,0]\n",
    "    TN=df.iloc[1,1]\n",
    "    FP=df.iloc[0,1]\n",
    "    FN=df.iloc[1,0]\n",
    "    acc=(TP+TN)/(TP+FN+FP+TN)\n",
    "    return(acc)\n",
    "\n",
    "print('El accuracy del clasificador bayesiano es de', accuracy(matriz_confusión_bayesiano).round(2))\n",
    "print('El accuracy del SVM es de', accuracy(matriz_confusión_svm).round(2))\n",
    "print('El accuracy del arbol de decisión es de', accuracy(matriz_confusión_arbol).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba545e4",
   "metadata": {},
   "source": [
    "El término accuracy hace referencia a lo cerca que está el resultado de una medición del valor verdadero. Estadísticamente, está relacionada con el sesgo de una estimación. Para su cálculo, se dividirán los resultados verdaderos (tanto verdaderos positivos (VP) como verdaderos negativos (VN)) entre el número total de casos examinados (verdaderos positivos, falsos positivos, verdaderos negativos, falsos negativos). Como se puede observar, el modelo con mayor cantidad de predicciones positivas que fueron correctas (mayor exactitud) es el SVM, seguido del arbol de decisión y del clasificador bayesiano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62733c",
   "metadata": {},
   "source": [
    "## 6. (Pregunta convalidable por la actividad de topic modelling. Confirmar con la profesora si está convalidado). Conteste a las siguientes preguntas basándote en evidencias de código. ¿Tiene influencia en el resultado final el número máximo de features a utilizar? ¿Modifica el resultado si se eliminan las stop words? ¿Y si se utilizan las formas canónicas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be09633",
   "metadata": {},
   "source": [
    "#### Sin max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d1d912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "      <th>žö</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "0  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "1  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "2  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "3  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "4  0.0  0.0     0.0           0.0   0.0   0.0          0.0          0.0   \n",
       "\n",
       "   0125698789   02  ...   û_  û_thanks  ûªm  ûªt  ûªve   ûï  ûïharry   ûò  \\\n",
       "0         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "1         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "2         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "3         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "4         0.0  0.0  ...  0.0       0.0  0.0  0.0   0.0  0.0      0.0  0.0   \n",
       "\n",
       "   ûówell   žö  \n",
       "0     0.0  0.0  \n",
       "1     0.0  0.0  \n",
       "2     0.0  0.0  \n",
       "3     0.0  0.0  \n",
       "4     0.0  0.0  \n",
       "\n",
       "[5 rows x 8673 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidf_max_feature(datos):\n",
    "    TFIDF_vectorizer = TfidfVectorizer(max_features=20)\n",
    "    TFIDF_array=TFIDF_vectorizer.fit_transform(datos).toarray()\n",
    "    vocab_TFIDF = TFIDF_vectorizer.get_feature_names()\n",
    "    TFIDF=pd.DataFrame(TFIDF_array, columns=vocab_TFIDF)\n",
    "    return(TFIDF.head(5))\n",
    "\n",
    "tfidf(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3efed",
   "metadata": {},
   "source": [
    "#### Con max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48800234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>can</th>\n",
       "      <th>for</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>me</th>\n",
       "      <th>my</th>\n",
       "      <th>now</th>\n",
       "      <th>of</th>\n",
       "      <th>on</th>\n",
       "      <th>so</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.406394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.913698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  are  call  can  for  have        in   is   it   me   my  now   of  \\\n",
       "0  0.0  0.0   0.0  0.0  0.0   0.0  1.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0   0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0   0.0  0.0  0.0   0.0  0.406394  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0   0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0   0.0  0.0  0.0   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    on   so  that  the        to  you  your  \n",
       "0  0.0  0.0   0.0  0.0  0.000000  0.0   0.0  \n",
       "1  0.0  0.0   0.0  0.0  0.000000  0.0   0.0  \n",
       "2  0.0  0.0   0.0  0.0  0.913698  0.0   0.0  \n",
       "3  0.0  1.0   0.0  0.0  0.000000  0.0   0.0  \n",
       "4  0.0  0.0   0.0  0.0  1.000000  0.0   0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max_feature(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd46d44",
   "metadata": {},
   "source": [
    "Si aplicamos un número máximo de \"features\" se puede observar como obtenemos una matriz que nos indica la frecuencia de términos mas familiares que la matriz original la cual contenía términos sin sentido. Aquí podemos observar palabras conocidas las cuales la gran mayoría son preposiciones, artículos o pronombres. Llama la atención cómo la palabra \"in\" o \"to\" tienen una frecuencia de 1. A priori, esta matriz no nos proporciona gran información. \n",
    "\n",
    "Es importante destacar que sólo aparecen las primeras 5 filas ya que he aplicado la función \"head\" a la función que nos devuelve esta matriz para tener una presentación más limpia. Es por eso por lo que la gran mayoría de las frecuencias es de 0, ya que sólo aparecen los 5 primeros documentos del corpus. Si representásemos la matriz entera, estas frecuencias aparecerían más altas en otras filas de la matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda659f",
   "metadata": {},
   "source": [
    "#### Sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1857af60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>free</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>gt</th>\n",
       "      <th>it</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>ll</th>\n",
       "      <th>love</th>\n",
       "      <th>lt</th>\n",
       "      <th>me</th>\n",
       "      <th>no</th>\n",
       "      <th>now</th>\n",
       "      <th>ok</th>\n",
       "      <th>time</th>\n",
       "      <th>ur</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   call  come  day  free  good  got   gt   it  know  like   ll  love   lt  \\\n",
       "0   0.0   0.0  0.0   0.0   0.0  1.0  0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "1   0.0   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "2   0.0   0.0  0.0   1.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "3   0.0   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "4   0.0   0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "    me   no  now   ok  time   ur  you  \n",
       "0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  1.0   0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0   0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0   0.0  0.0  0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max_feature(datos_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd385347",
   "metadata": {},
   "source": [
    "Si eliminamos las stop words, observamos cómo la matriz tfidf nos muestra la frecuencia de términos más comunes. Se pueden observar por lo general palabras de caracter mas positivo como \"good\" o \"love\". Llama la atención la columna que contiene el término \"ll\". Este es la abreviación del verbo \"will\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d4123",
   "metadata": {},
   "source": [
    "#### Formas canónicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66cb7fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>call</th>\n",
       "      <th>can</th>\n",
       "      <th>do</th>\n",
       "      <th>for</th>\n",
       "      <th>get</th>\n",
       "      <th>go</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>it</th>\n",
       "      <th>my</th>\n",
       "      <th>not</th>\n",
       "      <th>of</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>will</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588717</td>\n",
       "      <td>0.605517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.911963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and   be  call  can        do  for       get        go  have        in  \\\n",
       "0  0.0  0.0   0.0  0.0  0.000000  0.0  0.588717  0.605517   0.0  0.535501   \n",
       "1  0.0  0.0   0.0  0.0  0.000000  0.0  0.000000  0.000000   0.0  0.000000   \n",
       "2  0.0  0.0   0.0  0.0  0.000000  0.0  0.000000  0.000000   0.0  0.410273   \n",
       "3  0.0  0.0   0.0  0.0  0.000000  0.0  0.000000  0.000000   0.0  0.000000   \n",
       "4  0.0  0.0   0.0  0.0  0.508159  0.0  0.000000  0.590192   0.0  0.000000   \n",
       "\n",
       "    it   my       not   of  that  the        to  will  you  your  \n",
       "0  0.0  0.0  0.000000  0.0   0.0  0.0  0.000000   0.0  0.0   0.0  \n",
       "1  0.0  0.0  0.000000  0.0   0.0  0.0  0.000000   0.0  0.0   0.0  \n",
       "2  0.0  0.0  0.000000  0.0   0.0  0.0  0.911963   0.0  0.0   0.0  \n",
       "3  0.0  0.0  0.000000  0.0   0.0  0.0  0.000000   0.0  0.0   0.0  \n",
       "4  0.0  0.0  0.493849  0.0   0.0  0.0  0.386732   0.0  0.0   0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max_feature(formas_canonicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f19717",
   "metadata": {},
   "source": [
    "Si realizamos la matriz con las formas canónicas también vemos una gran mejoría en el resultado ya que se puede observar palabras conocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c5b66",
   "metadata": {},
   "source": [
    "#### Sin stopwords y con formas canónicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c9f4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>free</th>\n",
       "      <th>good</th>\n",
       "      <th>gt</th>\n",
       "      <th>home</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>lt</th>\n",
       "      <th>need</th>\n",
       "      <th>ok</th>\n",
       "      <th>send</th>\n",
       "      <th>tell</th>\n",
       "      <th>text</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>txt</th>\n",
       "      <th>ur</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   come  day     free  good   gt  home  know  like  love   lt  need   ok  \\\n",
       "0   0.0  0.0  0.00000   0.0  0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "1   0.0  0.0  0.00000   0.0  0.0   0.0   0.0   0.0   0.0  0.0   0.0  1.0   \n",
       "2   0.0  0.0  0.55623   0.0  0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "3   0.0  0.0  0.00000   0.0  0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "4   0.0  0.0  0.00000   0.0  0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0   \n",
       "\n",
       "   send  tell     text  think  time       txt   ur  want  \n",
       "0   0.0   0.0  0.00000    0.0   0.0  0.000000  0.0   0.0  \n",
       "1   0.0   0.0  0.00000    0.0   0.0  0.000000  0.0   0.0  \n",
       "2   0.0   0.0  0.57546    0.0   0.0  0.599544  0.0   0.0  \n",
       "3   0.0   0.0  0.00000    0.0   0.0  0.000000  0.0   0.0  \n",
       "4   0.0   0.0  0.00000    1.0   0.0  0.000000  0.0   0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_formas_canonicas=pd.DataFrame(formas_canonicas)\n",
    "formas_canonicas = df_formas_canonicas[0].squeeze()\n",
    "canonicas_stopwords=list(formas_canonicas.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)])))\n",
    "tfidf_max_feature(canonicas_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cd139",
   "metadata": {},
   "source": [
    "Si realizamos la matrif tfidf sin tener en cuenta las stopwords y con las formas canónicas, el resultado en comparación con la matriz original es mucho mas claro y se puede obtener mas valor. Sin embargo, no se puede obtener mucha más información con respecto a la matriz tfidf que no tiene en cuenta las stopwords ya que estas son muy similares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343c3d7",
   "metadata": {},
   "source": [
    "La conclusión final es que el pre-procesamiento realizado en el ejercicio 2 mejora mucho los resultados de la matriz tfidf. Hemos realizado 3 tipos de matrices tfidf:\n",
    "\n",
    "1. Sin stopwords\n",
    "2. Contando solo las formas canonicas\n",
    "3. Sin stopwords y contando solo las formas canonicas\n",
    "\n",
    "La tercera es la que mas información valiosa aporta, seguida de la primera y finalmente la segunda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c30c97",
   "metadata": {},
   "source": [
    "## 7. Imagínese que este entregable es una labor que le han solicitado en un entorno profesional, y que tiene que entregar esta documentación para comentar lo que ha descubierto (datos de entrada, rendimiento de los modelos, o cualquier descubrimiento que pueda ser importante). Comente los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e3145",
   "metadata": {},
   "source": [
    "Tras haber realizado la práctica, podemos llegar a las siguientes conclusiones:\n",
    "\n",
    "- Nuestro conjunto de datos contiene múltiples mensajes de spam y de no spam (usuarios) los cuales contienen caracteres extraños que nos dificultan la tarea. Por ello, es necesario realizar un preprocesamiento para obtener resultados lógicos\n",
    "- Dicho pre-procesamiento ha mejorado mucho los resultados de las matrices tfidf.\n",
    "- De los tres modelos, el mejor es el SVM en términos de exactitud. Es importante destacar que de los 3, el clasificador bayesiano es el que menos falsos positivos tiene.\n",
    "- Tras obtener las matrices tfidf teniendo en cuenta las formas canónicas y eliminando los stopwords, no se puede obtener información muy concreta. Por lo general, tanto los mensajes de spam como de no spam no muestra una actitud negativo sino más neutral o incluso positiva. Prueba de ello es la aperiencia de palabras como \"good\", \"free\" o \"love\" en la matriz.\n",
    "\n",
    "Para añadir mas valor, me gustaría explicar una idea que me ha surgido para mejorar los resultados. Realizando un análisis de los datos, he descubierto que tanto los mensajes spam como los que no lo son contienen una serie de caracteres extraños que nos aportan ruido a nuestras conclusiones. Una buena idea sería centrarse en los mensajes que no son spam y eliminar todos estos caracteres y números, de tal manera que nos quedemos con mensajes limpios escritos por usuarios. Esta solución que propongo sería parte del pre-procesamiento y lo haría de la siguiente forma:\n",
    "\n",
    "1. Realizar una función que nos indique cuales son todos los caracteres únicos en nuestro corpus de documentos\n",
    "2. Una vez tenemos estos caracteres en pantalla, nos fijamos en aquellos que queremos eliminar\n",
    "3. Aplicamos una función que elimina estos caracteres\n",
    "\n",
    "Pese a que este problema se podría solucionar obteniendo la forma canónica de la palabra, esto se podría ver afectado si estos caracteres se encontrasen en mitad de la palabra. Por ejemplo, si la palabra \"hola\" apareciese en algún mensaje como \"h#ola\", alomejor la función que obtuviese la forma canónica no funcionaría. Puesto que tenemos mas de 5000 documentos y no podemos ir uno a uno fijándonos en esto ya que queremos optimizar el tiempo, una buena solución sería esta que propongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d836f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', 'k', '\\r', 'ä', ']', '|', '+', 'q', 'ö', '4', '#', 'ò', \"'\",\n",
       "       'l', 'n', '-', 'm', '©', '‹', '÷', 'h', 'ó', '6', 'ž', '8', 'ð',\n",
       "       '´', '0', '/', 'f', 'ª', '@', '?', '*', ' ', 'è', 'e', 'å', 'b',\n",
       "       'o', '=', 'w', 'a', '(', 'c', 'v', '%', '&', 'ï', 'r', '7', '¬',\n",
       "       '‰', '~', 'x', ':', 'u', '^', 'õ', 'â', '1', 'ì', ',', 'y', ')',\n",
       "       's', 'p', 'z', 'û', '3', '2', '£', '!', '_', '5', '[', 'd', 'ô',\n",
       "       'g', '$', 'j', '\"', 'i', 't', '\\\\', '9', ';'], dtype='<U1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extra=data[data['label']=='ham']\n",
    "data_extra = data_extra.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "lista_minusculas=list(data_extra['text'])\n",
    "lista_unica=(\" \".join(lista_minusculas))\n",
    "lista_unica=list(set(lista_unica))\n",
    "\n",
    "array_unica=np.array(lista_unica).T\n",
    "array_unica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "371b21b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', '\\r', 'ä', ']', '|', '+', 'ö', '#', 'ò', \"'\", '-', '©', '‹',\n",
       "       '÷', 'ó', 'ž', 'ð', '´', '0', '/', 'ª', '@', '?', '*', 'è', 'å',\n",
       "       '=', '(', '%', '&', 'ï', '¬', '‰', '~', ':', '^', 'õ', 'â', 'ì',\n",
       "       ',', ')', 'û', '£', '!', '_', '[', 'ô', '$', '\"', '\\\\', ';'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_remove = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',' ','1','2','3','4','5','6','7','8','9']\n",
    "for n in num_to_remove:\n",
    "    while n in lista_unica:\n",
    "        lista_unica.remove(n)\n",
    "\n",
    "array_caracteres=np.array(lista_unica).T\n",
    "array_caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab761f3",
   "metadata": {},
   "source": [
    "En primer lugar, creo un nuevo dataframe el cual sólo contiene los mensajes de usuarios. Posteriormente, obtenemos de este nuevo dataframe todos los caracteres únicos. Finalmente, nos quedamos sólo con las letras del abecedario y los espacios en una lista y el resto de caracteres en otra. Es importante destacar que esto es simplemente un concepto inicial, y que se podría ir detallando más añadiendo símbolos como los del dinero ($,€) o los de operaciones (+,-) que pueden indicar gusto o disgusto del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae413005",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65e29762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase original es: I‰Û÷m going to try for 2 months ha ha only joking\n",
      "La frase modificada es: im going to try for 2 months ha ha only joking\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "characters = ['û', 'ð', ')', '_', '0', '-', '\\r', '´', ':', '£', '÷', \"'\", '&',\n",
    "       'ô', 'ª', '(', 'ì', '%', '\\\\', ',', 'è', 'ä', 'ï', '/', '\"', '+',\n",
    "       '@', 'ž', ';', 'õ', '=', '‹', '©', '$', 'ò', '¬', '?', '‰', '~',\n",
    "       '|', 'å', '!', '#', '.', 'ö', '[', '*', ']', 'â', 'ó', '^']\n",
    "for n in characters:\n",
    "    while n in data_extra['text'][21]:\n",
    "        data_extra['text'][21]=re.sub(n,\"\",data_extra['text'][21])\n",
    "        \n",
    "print('La frase original es:', data['text'][21])        \n",
    "print('La frase modificada es:', data_extra['text'][21])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fb917",
   "metadata": {},
   "source": [
    "Como se puede observar, la frase modificada cambia mucho y es mucho más sencilla de analizar. Si aplicamos una función la cual realice esta operación del ejemplo para todo nuestro corpus, seríamos capaces de obtener unos datos mucho más limpios que nos facilitarían a la hora de obtener resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
